{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcc544e15c0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # Implement Function\n",
    "    return np.array(x/255)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # Create the encoder\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    # Here the encoder finds the classes and assigns one-hot vectors \n",
    "    lb.fit(range(10))\n",
    "\n",
    "    return lb.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    batchSize = None\n",
    "    x = tf.placeholder(tf.float32,shape=[batchSize,*image_shape],name='x')    \n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    batchSize = None\n",
    "    y = tf.placeholder(tf.float32,shape=[batchSize,n_classes],name='y') \n",
    "    return y\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "    return keep_prob\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # Create the weight and bias using conv_ksize, conv_num_outputs and the shape of x_tensor.\n",
    "    tensor_shape = x_tensor.get_shape().as_list()\n",
    "    weight = tf.Variable(tf.truncated_normal(shape=[*conv_ksize,tensor_shape[3],conv_num_outputs],stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    #Apply a convolution to x_tensor using weight and conv_strides. Same padding is recommended\n",
    "    conv_layer = tf.nn.conv2d(x_tensor,weight,[1,*conv_strides,1],padding='SAME')\n",
    "    \n",
    "    #Add bias\n",
    "    conv_layer = tf.nn.bias_add(conv_layer,bias)\n",
    "    \n",
    "    #Add a nonlinear activation to the convolution.\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    #Apply Max Pooling using pool_ksize and pool_strides.Same padding is recommended\n",
    "    conv_layer=tf.nn.max_pool(conv_layer,[1,*pool_ksize,1],[1,*pool_strides,1],padding='SAME')\n",
    "    \n",
    "    return conv_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    tensor_shape = x_tensor.get_shape().as_list()\n",
    "    return tf.reshape(x_tensor, [-1, tensor_shape[1]*tensor_shape[2]*tensor_shape[3]])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    tensor_shape = x_tensor.get_shape().as_list()\n",
    "    num_inputs = tensor_shape[1]\n",
    "    weight = tf.Variable(tf.truncated_normal(shape=(num_inputs,num_outputs),stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    full_layer = tf.matmul(x_tensor,weight)\n",
    "    full_layer = tf.nn.bias_add(full_layer,bias)\n",
    "    full_layer = tf.nn.relu(full_layer)\n",
    "    return full_layer\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    tensor_shape = x_tensor.get_shape().as_list()\n",
    "    num_inputs = tensor_shape[1]\n",
    "    weight = tf.Variable(tf.truncated_normal(shape=(num_inputs,num_outputs),stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    output_layer = tf.matmul(x_tensor,weight)\n",
    "    output_layer = tf.nn.bias_add(output_layer,bias)\n",
    "    return output_layer\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    # 1 - Convolution and Max Pool layers\n",
    "    conv_num_outputs = 16\n",
    "    conv_ksize = (3,3)\n",
    "    conv_strides = (1,1)\n",
    "    pool_ksize = (2,2)\n",
    "    pool_strides = (2,2)\n",
    "    \n",
    "    conv_net1 = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    # 2 - Convolution and Max Pool layers\n",
    "    conv_num_outputs = 32\n",
    "    conv_net2 = conv2d_maxpool(conv_net1, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    # 3 - Convolution and Max Pool layers\n",
    "    conv_num_outputs = 64\n",
    "    conv_net3 = conv2d_maxpool(conv_net2, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    # Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat = flatten(conv_net3)\n",
    "\n",
    "    # Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    conn1_layer = fully_conn(flat, 512)\n",
    "    conn1_layer = tf.nn.dropout(conn1_layer,keep_prob = keep_prob)\n",
    "    \n",
    "    conn2_layer = fully_conn(conn1_layer, 512)\n",
    "    conn2_layer = tf.nn.dropout(conn2_layer,keep_prob = keep_prob)\n",
    "\n",
    "    \n",
    "    # Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    num_outputs = 10\n",
    "    output_layer = output(conn2_layer, num_outputs)\n",
    "    \n",
    "    # return output\n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer,feed_dict = {x:feature_batch, y:label_batch, keep_prob:keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    loss = session.run(cost, feed_dict = {x:feature_batch, y:label_batch, keep_prob:1.0})\n",
    "    training_accuracy = session.run(accuracy, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0})\n",
    "    validation_accuracy = session.run(accuracy, feed_dict = {x:valid_features, y:valid_labels, keep_prob:1.0})\n",
    "    \n",
    "    print('Loss:: {}'.format(loss),'Training Accuracy:: {}'.format(training_accuracy),'Validation Accuracy:: {}'.format(validation_accuracy))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 50\n",
    "batch_size = 512\n",
    "keep_probability = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:: 2.090961217880249 Training Accuracy:: 0.25337839126586914 Validation Accuracy:: 0.23579996824264526\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:: 1.8671326637268066 Training Accuracy:: 0.3547297418117523 Validation Accuracy:: 0.3673999607563019\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:: 1.7295751571655273 Training Accuracy:: 0.4054054021835327 Validation Accuracy:: 0.40379998087882996\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:: 1.626479148864746 Training Accuracy:: 0.45270273089408875 Validation Accuracy:: 0.4267999827861786\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:: 1.5186257362365723 Training Accuracy:: 0.45270270109176636 Validation Accuracy:: 0.4545999765396118\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:: 1.4410477876663208 Training Accuracy:: 0.4797297418117523 Validation Accuracy:: 0.4603999853134155\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:: 1.3520567417144775 Training Accuracy:: 0.5101351737976074 Validation Accuracy:: 0.4795999526977539\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:: 1.2643561363220215 Training Accuracy:: 0.5540540814399719 Validation Accuracy:: 0.5071999430656433\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:: 1.1562484502792358 Training Accuracy:: 0.625 Validation Accuracy:: 0.5260000228881836\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:: 1.1065449714660645 Training Accuracy:: 0.6520270705223083 Validation Accuracy:: 0.5229999423027039\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:: 1.0098646879196167 Training Accuracy:: 0.6925675868988037 Validation Accuracy:: 0.5433999300003052\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:: 0.9207181334495544 Training Accuracy:: 0.7162162065505981 Validation Accuracy:: 0.5565999150276184\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:: 0.8606213331222534 Training Accuracy:: 0.7263513803482056 Validation Accuracy:: 0.5531999468803406\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:: 0.7852588891983032 Training Accuracy:: 0.7702703475952148 Validation Accuracy:: 0.5583999752998352\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:: 0.7208218574523926 Training Accuracy:: 0.8074324727058411 Validation Accuracy:: 0.5659999251365662\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:: 0.6455812454223633 Training Accuracy:: 0.8108108043670654 Validation Accuracy:: 0.5699999332427979\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:: 0.589705765247345 Training Accuracy:: 0.8209459185600281 Validation Accuracy:: 0.5745999813079834\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:: 0.522619903087616 Training Accuracy:: 0.8513513803482056 Validation Accuracy:: 0.5709999203681946\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:: 0.4782148599624634 Training Accuracy:: 0.8682433366775513 Validation Accuracy:: 0.5771999359130859\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:: 0.4233803153038025 Training Accuracy:: 0.8817567825317383 Validation Accuracy:: 0.56659996509552\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:: 0.3714028596878052 Training Accuracy:: 0.8918918371200562 Validation Accuracy:: 0.5753999352455139\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:: 0.3454156517982483 Training Accuracy:: 0.9087837934494019 Validation Accuracy:: 0.5759998559951782\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:: 0.28892040252685547 Training Accuracy:: 0.9222972989082336 Validation Accuracy:: 0.58079993724823\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:: 0.28026264905929565 Training Accuracy:: 0.9560810327529907 Validation Accuracy:: 0.5715999007225037\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:: 0.26586657762527466 Training Accuracy:: 0.9222972989082336 Validation Accuracy:: 0.5685999393463135\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:: 0.21575140953063965 Training Accuracy:: 0.9391891956329346 Validation Accuracy:: 0.5773999691009521\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:: 0.17406998574733734 Training Accuracy:: 0.9729729890823364 Validation Accuracy:: 0.58079993724823\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:: 0.16651678085327148 Training Accuracy:: 0.962837815284729 Validation Accuracy:: 0.5745999217033386\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:: 0.17887382209300995 Training Accuracy:: 0.9695945978164673 Validation Accuracy:: 0.5583999752998352\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:: 0.14919009804725647 Training Accuracy:: 0.9729729890823364 Validation Accuracy:: 0.5639999508857727\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:: 0.1658884584903717 Training Accuracy:: 0.962837815284729 Validation Accuracy:: 0.5681999921798706\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:: 0.12880580127239227 Training Accuracy:: 0.9763513803482056 Validation Accuracy:: 0.5761998891830444\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:: 0.08076651394367218 Training Accuracy:: 0.986486554145813 Validation Accuracy:: 0.5817998647689819\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:: 0.08571840077638626 Training Accuracy:: 0.9898649454116821 Validation Accuracy:: 0.574199914932251\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:: 0.05936957523226738 Training Accuracy:: 0.9932433366775513 Validation Accuracy:: 0.5839999914169312\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:: 0.07924243062734604 Training Accuracy:: 0.9966216683387756 Validation Accuracy:: 0.5619999170303345\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:: 0.06098056584596634 Training Accuracy:: 0.9932432174682617 Validation Accuracy:: 0.571199893951416\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:: 0.09477639198303223 Training Accuracy:: 0.9797297716140747 Validation Accuracy:: 0.5657999515533447\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:: 0.059444114565849304 Training Accuracy:: 1.0 Validation Accuracy:: 0.5801999568939209\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:: 0.043803323060274124 Training Accuracy:: 0.9966216087341309 Validation Accuracy:: 0.5903999209403992\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:: 0.053886838257312775 Training Accuracy:: 1.0 Validation Accuracy:: 0.579599916934967\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:: 0.04709898307919502 Training Accuracy:: 0.9966216087341309 Validation Accuracy:: 0.5757999420166016\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:: 0.057366933673620224 Training Accuracy:: 0.9864864349365234 Validation Accuracy:: 0.5669999122619629\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:: 0.04853126034140587 Training Accuracy:: 0.9932432174682617 Validation Accuracy:: 0.5823999047279358\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:: 0.03230538219213486 Training Accuracy:: 1.0 Validation Accuracy:: 0.5903998613357544\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:: 0.05333791673183441 Training Accuracy:: 0.9932433366775513 Validation Accuracy:: 0.5871999263763428\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:: 0.021819567307829857 Training Accuracy:: 1.0 Validation Accuracy:: 0.5819999575614929\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:: 0.035028405487537384 Training Accuracy:: 0.9932433366775513 Validation Accuracy:: 0.5755999088287354\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:: 0.031368792057037354 Training Accuracy:: 0.9966216087341309 Validation Accuracy:: 0.5787999629974365\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:: 0.031443171203136444 Training Accuracy:: 0.9966216087341309 Validation Accuracy:: 0.5643998980522156\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:: 2.111250638961792 Training Accuracy:: 0.27027028799057007 Validation Accuracy:: 0.26019996404647827\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:: 1.735640287399292 Training Accuracy:: 0.37837839126586914 Validation Accuracy:: 0.36959996819496155\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:: 1.4857760667800903 Training Accuracy:: 0.5000000596046448 Validation Accuracy:: 0.4129999577999115\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:: 1.4979764223098755 Training Accuracy:: 0.45945948362350464 Validation Accuracy:: 0.4453999400138855\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:: 1.4529430866241455 Training Accuracy:: 0.4729729890823364 Validation Accuracy:: 0.4615999460220337\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:: 1.5287977457046509 Training Accuracy:: 0.4628378450870514 Validation Accuracy:: 0.47940000891685486\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:: 1.2977250814437866 Training Accuracy:: 0.5405405163764954 Validation Accuracy:: 0.5037999749183655\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:: 1.2215205430984497 Training Accuracy:: 0.5709459781646729 Validation Accuracy:: 0.4965999722480774\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:: 1.2129950523376465 Training Accuracy:: 0.5439189672470093 Validation Accuracy:: 0.525399923324585\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:: 1.1966397762298584 Training Accuracy:: 0.5777027606964111 Validation Accuracy:: 0.5353999733924866\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:: 1.3262696266174316 Training Accuracy:: 0.5506756901741028 Validation Accuracy:: 0.539199948310852\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:: 1.1199957132339478 Training Accuracy:: 0.6351351737976074 Validation Accuracy:: 0.5583999156951904\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:: 1.0696380138397217 Training Accuracy:: 0.6216216087341309 Validation Accuracy:: 0.5475999116897583\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:: 1.0296183824539185 Training Accuracy:: 0.6418918967247009 Validation Accuracy:: 0.5829999446868896\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:: 1.0115585327148438 Training Accuracy:: 0.6587837934494019 Validation Accuracy:: 0.5883998870849609\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:: 1.1672930717468262 Training Accuracy:: 0.6182432770729065 Validation Accuracy:: 0.5941998958587646\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:: 0.9731457233428955 Training Accuracy:: 0.6891891956329346 Validation Accuracy:: 0.5991998910903931\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:: 0.9004329442977905 Training Accuracy:: 0.6790541410446167 Validation Accuracy:: 0.5905999541282654\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:: 0.885913074016571 Training Accuracy:: 0.712837815284729 Validation Accuracy:: 0.6129999160766602\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:: 0.8953771591186523 Training Accuracy:: 0.6722973585128784 Validation Accuracy:: 0.6143999099731445\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:: 1.009894609451294 Training Accuracy:: 0.6858108639717102 Validation Accuracy:: 0.6317999362945557\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:: 0.8825315237045288 Training Accuracy:: 0.7060810923576355 Validation Accuracy:: 0.6213998794555664\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:: 0.8336611986160278 Training Accuracy:: 0.7128378748893738 Validation Accuracy:: 0.6141998767852783\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:: 0.8250485062599182 Training Accuracy:: 0.699324369430542 Validation Accuracy:: 0.6305999159812927\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:: 0.7869561910629272 Training Accuracy:: 0.7263513803482056 Validation Accuracy:: 0.6481999158859253\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:: 0.8980777859687805 Training Accuracy:: 0.6959459781646729 Validation Accuracy:: 0.6393998861312866\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:: 0.769681990146637 Training Accuracy:: 0.7533783912658691 Validation Accuracy:: 0.6401998996734619\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:: 0.7379063367843628 Training Accuracy:: 0.7398648858070374 Validation Accuracy:: 0.6365998983383179\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:: 0.6891987919807434 Training Accuracy:: 0.7466216683387756 Validation Accuracy:: 0.6531999111175537\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:: 0.7095931172370911 Training Accuracy:: 0.7466216683387756 Validation Accuracy:: 0.6637998819351196\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:: 0.8297557830810547 Training Accuracy:: 0.736486554145813 Validation Accuracy:: 0.6609998345375061\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:: 0.7198456525802612 Training Accuracy:: 0.7770270705223083 Validation Accuracy:: 0.6397998929023743\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:: 0.6664958596229553 Training Accuracy:: 0.7770270705223083 Validation Accuracy:: 0.6521998643875122\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:: 0.6639369130134583 Training Accuracy:: 0.7804054021835327 Validation Accuracy:: 0.6533998250961304\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:: 0.6421553492546082 Training Accuracy:: 0.7702703475952148 Validation Accuracy:: 0.6663998365402222\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:: 0.7447845935821533 Training Accuracy:: 0.75 Validation Accuracy:: 0.6741998791694641\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:: 0.6355038285255432 Training Accuracy:: 0.8141891956329346 Validation Accuracy:: 0.6617999076843262\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:: 0.599673867225647 Training Accuracy:: 0.8074324727058411 Validation Accuracy:: 0.6721998453140259\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:: 0.5873731374740601 Training Accuracy:: 0.7972972989082336 Validation Accuracy:: 0.6735999584197998\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:: 0.6075141429901123 Training Accuracy:: 0.7804054617881775 Validation Accuracy:: 0.673399806022644\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:: 0.6521720886230469 Training Accuracy:: 0.7736486792564392 Validation Accuracy:: 0.6819998621940613\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:: 0.5455702543258667 Training Accuracy:: 0.8412163257598877 Validation Accuracy:: 0.6757998466491699\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:: 0.5351266860961914 Training Accuracy:: 0.8175675868988037 Validation Accuracy:: 0.6851998567581177\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:: 0.5178120136260986 Training Accuracy:: 0.8277027606964111 Validation Accuracy:: 0.6803998351097107\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:: 0.5285468101501465 Training Accuracy:: 0.8209459185600281 Validation Accuracy:: 0.6817998290061951\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:: 0.6097279787063599 Training Accuracy:: 0.800675630569458 Validation Accuracy:: 0.6857998967170715\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:: 0.4914287328720093 Training Accuracy:: 0.8513513803482056 Validation Accuracy:: 0.6899999380111694\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:: 0.5044963359832764 Training Accuracy:: 0.8412162661552429 Validation Accuracy:: 0.6805999279022217\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:: 0.46656930446624756 Training Accuracy:: 0.8547298312187195 Validation Accuracy:: 0.6925998330116272\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:: 0.44755467772483826 Training Accuracy:: 0.8479730486869812 Validation Accuracy:: 0.6885998249053955\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:: 0.5765390396118164 Training Accuracy:: 0.8040540814399719 Validation Accuracy:: 0.6775999069213867\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:: 0.48038336634635925 Training Accuracy:: 0.8445946574211121 Validation Accuracy:: 0.6815999150276184\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:: 0.44132232666015625 Training Accuracy:: 0.8547297120094299 Validation Accuracy:: 0.687799870967865\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:: 0.41105201840400696 Training Accuracy:: 0.8648648858070374 Validation Accuracy:: 0.6953999400138855\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:: 0.41888427734375 Training Accuracy:: 0.8885135054588318 Validation Accuracy:: 0.6877998113632202\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:: 0.5599576234817505 Training Accuracy:: 0.8175675868988037 Validation Accuracy:: 0.6673998832702637\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:: 0.42928823828697205 Training Accuracy:: 0.8682432770729065 Validation Accuracy:: 0.6959998607635498\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:: 0.4065244793891907 Training Accuracy:: 0.8547297716140747 Validation Accuracy:: 0.6901998519897461\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:: 0.36714261770248413 Training Accuracy:: 0.8682432770729065 Validation Accuracy:: 0.7023998498916626\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:: 0.36573877930641174 Training Accuracy:: 0.8986485600471497 Validation Accuracy:: 0.6907998919487\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:: 0.4566258490085602 Training Accuracy:: 0.8513513207435608 Validation Accuracy:: 0.6945998668670654\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:: 0.39460062980651855 Training Accuracy:: 0.875 Validation Accuracy:: 0.7007998824119568\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:: 0.34490469098091125 Training Accuracy:: 0.8918919563293457 Validation Accuracy:: 0.7019999027252197\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:: 0.34143975377082825 Training Accuracy:: 0.9020270109176636 Validation Accuracy:: 0.6983999013900757\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:: 0.28088927268981934 Training Accuracy:: 0.9391891360282898 Validation Accuracy:: 0.6983997821807861\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:: 0.4016154110431671 Training Accuracy:: 0.8817567825317383 Validation Accuracy:: 0.7069998383522034\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:: 0.34775984287261963 Training Accuracy:: 0.8918918967247009 Validation Accuracy:: 0.7017998695373535\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:: 0.32210642099380493 Training Accuracy:: 0.9054054617881775 Validation Accuracy:: 0.7041998505592346\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:: 0.31262195110321045 Training Accuracy:: 0.898648738861084 Validation Accuracy:: 0.6957998275756836\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:: 0.2937416434288025 Training Accuracy:: 0.9222972989082336 Validation Accuracy:: 0.6887998580932617\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:: 0.3715413212776184 Training Accuracy:: 0.8918919563293457 Validation Accuracy:: 0.7011998295783997\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:: 0.33664125204086304 Training Accuracy:: 0.8817566633224487 Validation Accuracy:: 0.6853998899459839\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:: 0.30704668164253235 Training Accuracy:: 0.9189189076423645 Validation Accuracy:: 0.7107998728752136\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:: 0.27028003334999084 Training Accuracy:: 0.9155405759811401 Validation Accuracy:: 0.7059998512268066\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:: 0.2657122313976288 Training Accuracy:: 0.9155404567718506 Validation Accuracy:: 0.6921998858451843\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:: 0.33643242716789246 Training Accuracy:: 0.912162184715271 Validation Accuracy:: 0.709199845790863\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:: 0.2803349792957306 Training Accuracy:: 0.9256757497787476 Validation Accuracy:: 0.6957998275756836\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:: 0.2500942349433899 Training Accuracy:: 0.9155405759811401 Validation Accuracy:: 0.7087998390197754\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:: 0.26621362566947937 Training Accuracy:: 0.9087837934494019 Validation Accuracy:: 0.7001998424530029\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:: 0.2171889692544937 Training Accuracy:: 0.9290540814399719 Validation Accuracy:: 0.689599871635437\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:: 0.3010513186454773 Training Accuracy:: 0.912162184715271 Validation Accuracy:: 0.6983998417854309\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:: 0.2199430763721466 Training Accuracy:: 0.9358107447624207 Validation Accuracy:: 0.7131998538970947\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:: 0.2248956561088562 Training Accuracy:: 0.9391891956329346 Validation Accuracy:: 0.7063999176025391\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:: 0.20818093419075012 Training Accuracy:: 0.9358108639717102 Validation Accuracy:: 0.6967998147010803\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:: 0.20079781115055084 Training Accuracy:: 0.9324323534965515 Validation Accuracy:: 0.7029998898506165\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:: 0.29434892535209656 Training Accuracy:: 0.9155405759811401 Validation Accuracy:: 0.6899998784065247\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:: 0.20022554695606232 Training Accuracy:: 0.9493242502212524 Validation Accuracy:: 0.7123998403549194\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:: 0.2037559449672699 Training Accuracy:: 0.9324324727058411 Validation Accuracy:: 0.7157998085021973\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:: 0.19665226340293884 Training Accuracy:: 0.9493242502212524 Validation Accuracy:: 0.7049999237060547\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:: 0.181966170668602 Training Accuracy:: 0.9594594836235046 Validation Accuracy:: 0.6875999569892883\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:: 0.3006414473056793 Training Accuracy:: 0.9054054021835327 Validation Accuracy:: 0.6797999143600464\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:: 0.19222921133041382 Training Accuracy:: 0.9695945978164673 Validation Accuracy:: 0.7027998566627502\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:: 0.17566226422786713 Training Accuracy:: 0.9560811519622803 Validation Accuracy:: 0.7067998647689819\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:: 0.19717693328857422 Training Accuracy:: 0.9358108043670654 Validation Accuracy:: 0.6977998614311218\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:: 0.18457892537117004 Training Accuracy:: 0.9425675868988037 Validation Accuracy:: 0.6849998831748962\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:: 0.2532322406768799 Training Accuracy:: 0.9020270705223083 Validation Accuracy:: 0.7005999088287354\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:: 0.2168256640434265 Training Accuracy:: 0.9560810923576355 Validation Accuracy:: 0.6941999197006226\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:: 0.15369758009910583 Training Accuracy:: 0.9560811519622803 Validation Accuracy:: 0.716999888420105\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:: 0.14179947972297668 Training Accuracy:: 0.962837815284729 Validation Accuracy:: 0.7097998857498169\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:: 0.13564422726631165 Training Accuracy:: 0.9695945978164673 Validation Accuracy:: 0.6959998607635498\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:: 0.20769508183002472 Training Accuracy:: 0.9290540218353271 Validation Accuracy:: 0.7069998979568481\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:: 0.2202850580215454 Training Accuracy:: 0.9527026414871216 Validation Accuracy:: 0.6761999130249023\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:: 0.15631967782974243 Training Accuracy:: 0.9662162065505981 Validation Accuracy:: 0.7135998606681824\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:: 0.1391429901123047 Training Accuracy:: 0.9763513803482056 Validation Accuracy:: 0.7059998512268066\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:: 0.14770352840423584 Training Accuracy:: 0.9662162065505981 Validation Accuracy:: 0.6889998912811279\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:: 0.20268979668617249 Training Accuracy:: 0.9459459781646729 Validation Accuracy:: 0.7033998370170593\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:: 0.13958390057086945 Training Accuracy:: 0.9729729890823364 Validation Accuracy:: 0.6977998614311218\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:: 0.14636485278606415 Training Accuracy:: 0.962837815284729 Validation Accuracy:: 0.7165998220443726\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:: 0.13449600338935852 Training Accuracy:: 0.9594594240188599 Validation Accuracy:: 0.7021998167037964\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:: 0.14069423079490662 Training Accuracy:: 0.9729729890823364 Validation Accuracy:: 0.6851999163627625\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:: 0.1901559829711914 Training Accuracy:: 0.9425674676895142 Validation Accuracy:: 0.7107999324798584\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:: 0.11861348152160645 Training Accuracy:: 0.9898648858070374 Validation Accuracy:: 0.7075998783111572\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:: 0.12113840878009796 Training Accuracy:: 0.9729729890823364 Validation Accuracy:: 0.7093998193740845\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:: 0.08851730078458786 Training Accuracy:: 0.9898648858070374 Validation Accuracy:: 0.7237998843193054\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:: 0.11165256053209305 Training Accuracy:: 0.9831081628799438 Validation Accuracy:: 0.697799801826477\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:: 0.1620984673500061 Training Accuracy:: 0.9560810327529907 Validation Accuracy:: 0.7059998512268066\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:: 0.1141124740242958 Training Accuracy:: 0.986486554145813 Validation Accuracy:: 0.7093998789787292\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:: 0.10358868539333344 Training Accuracy:: 0.9729729890823364 Validation Accuracy:: 0.7075998783111572\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:: 0.08635222911834717 Training Accuracy:: 0.9864864945411682 Validation Accuracy:: 0.7129998207092285\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:: 0.0965258851647377 Training Accuracy:: 0.9831081628799438 Validation Accuracy:: 0.6929998397827148\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:: 0.12192441523075104 Training Accuracy:: 0.9763513803482056 Validation Accuracy:: 0.701999843120575\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:: 0.07333245128393173 Training Accuracy:: 0.9932433366775513 Validation Accuracy:: 0.7133998274803162\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:: 0.08641965687274933 Training Accuracy:: 0.9797297716140747 Validation Accuracy:: 0.7097998857498169\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:: 0.07755754888057709 Training Accuracy:: 0.9898648858070374 Validation Accuracy:: 0.7137999534606934\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:: 0.08370020240545273 Training Accuracy:: 0.9898649454116821 Validation Accuracy:: 0.7015998959541321\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:: 0.14087259769439697 Training Accuracy:: 0.9560811519622803 Validation Accuracy:: 0.6989999413490295\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:: 0.07841631025075912 Training Accuracy:: 0.9966216683387756 Validation Accuracy:: 0.7149998545646667\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:: 0.07795695215463638 Training Accuracy:: 0.9797297716140747 Validation Accuracy:: 0.7151998281478882\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:: 0.06925784796476364 Training Accuracy:: 0.9898649454116821 Validation Accuracy:: 0.7055999040603638\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:: 0.098879873752594 Training Accuracy:: 0.9763513803482056 Validation Accuracy:: 0.6939998865127563\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:: 0.1451496034860611 Training Accuracy:: 0.962837815284729 Validation Accuracy:: 0.6901998519897461\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:: 0.06926454603672028 Training Accuracy:: 0.9932432174682617 Validation Accuracy:: 0.7107998728752136\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:: 0.08206597715616226 Training Accuracy:: 0.9864864945411682 Validation Accuracy:: 0.7011998295783997\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:: 0.08389031141996384 Training Accuracy:: 0.986486554145813 Validation Accuracy:: 0.6877998113632202\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:: 0.0906309261918068 Training Accuracy:: 0.9831081628799438 Validation Accuracy:: 0.6943997740745544\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:: 0.1616537868976593 Training Accuracy:: 0.9527027010917664 Validation Accuracy:: 0.6867998242378235\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:: 0.0558273009955883 Training Accuracy:: 1.0 Validation Accuracy:: 0.715199887752533\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:: 0.05453796684741974 Training Accuracy:: 0.9898648262023926 Validation Accuracy:: 0.7145999073982239\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:: 0.06382322311401367 Training Accuracy:: 0.986486554145813 Validation Accuracy:: 0.7033998370170593\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:: 0.09086048603057861 Training Accuracy:: 0.9831081628799438 Validation Accuracy:: 0.68479984998703\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:: 0.11109022051095963 Training Accuracy:: 0.9695945978164673 Validation Accuracy:: 0.7001999020576477\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:: 0.05092532932758331 Training Accuracy:: 0.9932433366775513 Validation Accuracy:: 0.7071998119354248\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:: 0.04402875527739525 Training Accuracy:: 1.0 Validation Accuracy:: 0.7187998294830322\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:: 0.05367448925971985 Training Accuracy:: 0.9966216087341309 Validation Accuracy:: 0.6963998675346375\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:: 0.06931135058403015 Training Accuracy:: 0.9932433366775513 Validation Accuracy:: 0.6899998188018799\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:: 0.0955507680773735 Training Accuracy:: 0.9797297716140747 Validation Accuracy:: 0.7013999223709106\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:: 0.04345932975411415 Training Accuracy:: 0.9932432174682617 Validation Accuracy:: 0.7045998573303223\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:: 0.03616056218743324 Training Accuracy:: 1.0 Validation Accuracy:: 0.7189998626708984\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:: 0.06049954146146774 Training Accuracy:: 0.9932433366775513 Validation Accuracy:: 0.6973998546600342\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:: 0.0483304038643837 Training Accuracy:: 0.9932432770729065 Validation Accuracy:: 0.6971998810768127\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:: 0.07527781277894974 Training Accuracy:: 0.9898649454116821 Validation Accuracy:: 0.7001999020576477\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:: 0.048673275858163834 Training Accuracy:: 0.9932432174682617 Validation Accuracy:: 0.7151998281478882\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:: 0.044850725680589676 Training Accuracy:: 0.9932432174682617 Validation Accuracy:: 0.71319979429245\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:: 0.047034598886966705 Training Accuracy:: 0.9932433366775513 Validation Accuracy:: 0.7207998633384705\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:: 0.05311186984181404 Training Accuracy:: 0.9898648858070374 Validation Accuracy:: 0.6995998620986938\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:: 0.05020488426089287 Training Accuracy:: 0.9966216087341309 Validation Accuracy:: 0.7047998905181885\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:: 0.03311701491475105 Training Accuracy:: 1.0 Validation Accuracy:: 0.7199998497962952\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:: 0.029240772128105164 Training Accuracy:: 0.9966216087341309 Validation Accuracy:: 0.7245998978614807\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:: 0.026163902133703232 Training Accuracy:: 0.9966217279434204 Validation Accuracy:: 0.7207998633384705\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:: 0.040948957204818726 Training Accuracy:: 0.9898648262023926 Validation Accuracy:: 0.701999843120575\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:: 0.040068384259939194 Training Accuracy:: 0.9966216087341309 Validation Accuracy:: 0.7125998735427856\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:: 0.027034269645810127 Training Accuracy:: 1.0 Validation Accuracy:: 0.7167998552322388\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:: 0.021951761096715927 Training Accuracy:: 1.0 Validation Accuracy:: 0.7183998823165894\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:: 0.03041223995387554 Training Accuracy:: 0.9966216087341309 Validation Accuracy:: 0.707399845123291\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:: 0.02306569181382656 Training Accuracy:: 1.0 Validation Accuracy:: 0.7111998796463013\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:: 0.036730557680130005 Training Accuracy:: 0.9966217279434204 Validation Accuracy:: 0.7029998302459717\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:: 0.0209059901535511 Training Accuracy:: 1.0 Validation Accuracy:: 0.7143998146057129\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:: 0.02294200286269188 Training Accuracy:: 0.9966216087341309 Validation Accuracy:: 0.7123998403549194\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:: 0.026865556836128235 Training Accuracy:: 1.0 Validation Accuracy:: 0.7029998302459717\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:: 0.029992975294589996 Training Accuracy:: 0.9966216683387756 Validation Accuracy:: 0.7053998708724976\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:: 0.031270187348127365 Training Accuracy:: 0.9966216087341309 Validation Accuracy:: 0.7075998783111572\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:: 0.019759999588131905 Training Accuracy:: 1.0 Validation Accuracy:: 0.709199845790863\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:: 0.03344566747546196 Training Accuracy:: 0.9932433366775513 Validation Accuracy:: 0.7149999141693115\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:: 0.020902017131447792 Training Accuracy:: 0.9966216087341309 Validation Accuracy:: 0.7233998775482178\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:: 0.02059773914515972 Training Accuracy:: 1.0 Validation Accuracy:: 0.701999843120575\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:: 0.030122729018330574 Training Accuracy:: 0.9966216087341309 Validation Accuracy:: 0.7043998837471008\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:: 0.013560298830270767 Training Accuracy:: 1.0 Validation Accuracy:: 0.7129998207092285\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:: 0.023112157359719276 Training Accuracy:: 1.0 Validation Accuracy:: 0.7115998864173889\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:: 0.018210427835583687 Training Accuracy:: 0.9966216087341309 Validation Accuracy:: 0.7221998572349548\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:: 0.016607865691184998 Training Accuracy:: 1.0 Validation Accuracy:: 0.7211998701095581\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:: 0.02567940205335617 Training Accuracy:: 1.0 Validation Accuracy:: 0.7115998268127441\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:: 0.022548846900463104 Training Accuracy:: 1.0 Validation Accuracy:: 0.6977999210357666\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:: 0.020052844658493996 Training Accuracy:: 0.9966217279434204 Validation Accuracy:: 0.7143999338150024\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:: 0.019877798855304718 Training Accuracy:: 0.9966216683387756 Validation Accuracy:: 0.7175998687744141\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:: 0.013063411228358746 Training Accuracy:: 1.0 Validation Accuracy:: 0.7177999019622803\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:: 0.0273460503667593 Training Accuracy:: 0.9966217279434204 Validation Accuracy:: 0.71399986743927\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:: 0.01429549790918827 Training Accuracy:: 1.0 Validation Accuracy:: 0.7109998464584351\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:: 0.016444092616438866 Training Accuracy:: 1.0 Validation Accuracy:: 0.7175998687744141\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:: 0.017624352127313614 Training Accuracy:: 1.0 Validation Accuracy:: 0.7219998240470886\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:: 0.010863062925636768 Training Accuracy:: 1.0 Validation Accuracy:: 0.7213999032974243\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:: 0.02043132856488228 Training Accuracy:: 1.0 Validation Accuracy:: 0.7097998261451721\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:: 0.03306137025356293 Training Accuracy:: 0.9898649454116821 Validation Accuracy:: 0.7005998492240906\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:: 0.011955763213336468 Training Accuracy:: 1.0 Validation Accuracy:: 0.7145998477935791\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:: 0.01637846790254116 Training Accuracy:: 0.9966217279434204 Validation Accuracy:: 0.7079998254776001\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:: 0.018831372261047363 Training Accuracy:: 1.0 Validation Accuracy:: 0.7011998891830444\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:: 0.019430017098784447 Training Accuracy:: 0.9966216087341309 Validation Accuracy:: 0.7119998931884766\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:: 0.015120044350624084 Training Accuracy:: 1.0 Validation Accuracy:: 0.6995998620986938\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:: 0.01944660022854805 Training Accuracy:: 0.9966217279434204 Validation Accuracy:: 0.7113997936248779\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:: 0.008758730255067348 Training Accuracy:: 1.0 Validation Accuracy:: 0.7201998829841614\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:: 0.01228753849864006 Training Accuracy:: 1.0 Validation Accuracy:: 0.7163998484611511\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:: 0.017139509320259094 Training Accuracy:: 0.9966216087341309 Validation Accuracy:: 0.7175998687744141\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:: 0.018291212618350983 Training Accuracy:: 1.0 Validation Accuracy:: 0.7059999108314514\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:: 0.02774912118911743 Training Accuracy:: 0.9932432174682617 Validation Accuracy:: 0.694399893283844\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:: 0.01988985948264599 Training Accuracy:: 0.9966217279434204 Validation Accuracy:: 0.6983999013900757\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:: 0.014440211467444897 Training Accuracy:: 1.0 Validation Accuracy:: 0.7159999012947083\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:: 0.021196085959672928 Training Accuracy:: 1.0 Validation Accuracy:: 0.7141997814178467\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:: 0.015805033966898918 Training Accuracy:: 1.0 Validation Accuracy:: 0.6967998743057251\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:: 0.015024963766336441 Training Accuracy:: 1.0 Validation Accuracy:: 0.704399824142456\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:: 0.01788886822760105 Training Accuracy:: 0.9966216683387756 Validation Accuracy:: 0.7079998254776001\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:: 0.00914971623569727 Training Accuracy:: 1.0 Validation Accuracy:: 0.7121998071670532\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:: 0.018798207864165306 Training Accuracy:: 1.0 Validation Accuracy:: 0.7053998708724976\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:: 0.012147555127739906 Training Accuracy:: 0.9966216087341309 Validation Accuracy:: 0.7041999101638794\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:: 0.013921486213803291 Training Accuracy:: 1.0 Validation Accuracy:: 0.7077998518943787\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:: 0.010100241750478745 Training Accuracy:: 1.0 Validation Accuracy:: 0.7075998187065125\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:: 0.009837416000664234 Training Accuracy:: 1.0 Validation Accuracy:: 0.7119998931884766\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:: 0.014456532895565033 Training Accuracy:: 1.0 Validation Accuracy:: 0.7159998416900635\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:: 0.015829432755708694 Training Accuracy:: 1.0 Validation Accuracy:: 0.7053998112678528\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:: 0.011407575570046902 Training Accuracy:: 1.0 Validation Accuracy:: 0.7081998586654663\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:: 0.011132991872727871 Training Accuracy:: 1.0 Validation Accuracy:: 0.7105998992919922\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:: 0.014433840289711952 Training Accuracy:: 1.0 Validation Accuracy:: 0.7029998302459717\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:: 0.011447113007307053 Training Accuracy:: 1.0 Validation Accuracy:: 0.7081999182701111\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:: 0.012386644259095192 Training Accuracy:: 1.0 Validation Accuracy:: 0.704399824142456\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:: 0.009285198524594307 Training Accuracy:: 1.0 Validation Accuracy:: 0.7091999053955078\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:: 0.013019309379160404 Training Accuracy:: 1.0 Validation Accuracy:: 0.7077998518943787\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:: 0.008692096918821335 Training Accuracy:: 1.0 Validation Accuracy:: 0.7139998078346252\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:: 0.01836598664522171 Training Accuracy:: 0.9932432174682617 Validation Accuracy:: 0.7209998965263367\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:: 0.0069734095595777035 Training Accuracy:: 1.0 Validation Accuracy:: 0.7135998010635376\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:: 0.007410728372633457 Training Accuracy:: 1.0 Validation Accuracy:: 0.7055999040603638\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:: 0.013319551013410091 Training Accuracy:: 0.9966216087341309 Validation Accuracy:: 0.7121998071670532\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:: 0.010252661071717739 Training Accuracy:: 1.0 Validation Accuracy:: 0.7093997597694397\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:: 0.009428978897631168 Training Accuracy:: 0.9966216087341309 Validation Accuracy:: 0.7147998809814453\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:: 0.0043606385588645935 Training Accuracy:: 1.0 Validation Accuracy:: 0.7175998091697693\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:: 0.008321892470121384 Training Accuracy:: 1.0 Validation Accuracy:: 0.7005998492240906\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:: 0.02651376836001873 Training Accuracy:: 0.9932432770729065 Validation Accuracy:: 0.695399820804596\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:: 0.0064481464214622974 Training Accuracy:: 1.0 Validation Accuracy:: 0.7063998579978943\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:: 0.008461900055408478 Training Accuracy:: 1.0 Validation Accuracy:: 0.7075998783111572\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:: 0.005186889786273241 Training Accuracy:: 1.0 Validation Accuracy:: 0.7159998416900635\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:: 0.012336223386228085 Training Accuracy:: 1.0 Validation Accuracy:: 0.6867998242378235\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:: 0.0099859070032835 Training Accuracy:: 1.0 Validation Accuracy:: 0.7113998532295227\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:: 0.006976565811783075 Training Accuracy:: 1.0 Validation Accuracy:: 0.7105998396873474\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:: 0.007872164249420166 Training Accuracy:: 1.0 Validation Accuracy:: 0.718599796295166\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:: 0.00604474451392889 Training Accuracy:: 1.0 Validation Accuracy:: 0.7125998735427856\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:: 0.00630229152739048 Training Accuracy:: 1.0 Validation Accuracy:: 0.7021999359130859\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:: 0.0061471834778785706 Training Accuracy:: 1.0 Validation Accuracy:: 0.7015998363494873\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:: 0.004220122471451759 Training Accuracy:: 1.0 Validation Accuracy:: 0.7141998410224915\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:: 0.013738950714468956 Training Accuracy:: 0.9932433366775513 Validation Accuracy:: 0.7113999128341675\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:: 0.010398351587355137 Training Accuracy:: 1.0 Validation Accuracy:: 0.7147998213768005\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:: 0.006615419406443834 Training Accuracy:: 1.0 Validation Accuracy:: 0.7049998044967651\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:: 0.007318616379052401 Training Accuracy:: 1.0 Validation Accuracy:: 0.7137998938560486\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:: 0.005167369730770588 Training Accuracy:: 1.0 Validation Accuracy:: 0.7173998355865479\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7082605689764023\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArsAAAJ/CAYAAAB4GhsgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcca72939b0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 349
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
